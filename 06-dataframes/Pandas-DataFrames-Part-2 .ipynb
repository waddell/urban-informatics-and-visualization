{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Pandas DataFrames - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last session we introduced Pandas. Now we review the basics of using Pandas, with a more interactive workflow using relevant data to solidify understanding and facility with this powerful Python toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing Pandas Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/ca_tracts_pop_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit more practice with indexing in Pandas\n",
    "\n",
    "The first use of indexing is to use a slice, just like we have done with other Python objects. Below we slice the first 5 index values of the first dimension of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first indexing method is equivalent to usinf the iloc indexing method, which uses the integer based indexing, purely based on the location of the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, Pandas has a built-in short-cut method called 'head' that shows the first 5 rows of a dataframe.  Very handy when the dataframe is large to just look at a few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second way to index is using loc, which uses the labels of the index. Note that this approach includes the second value in the index range, whereas iloc does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that indexing can work for both rows and colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:5, : 'GEOID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:5, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select rows based on their value as well.  Notice that we nest df[df[condition]] to get this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Population'] < 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we find and print records that are in El Dorado County, using the str attribute and 'contains' to search for the county name in geodisplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['geodisplay'].str.contains('El Dorado County')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['GEOID']==6017031900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show how to set a value of a cell in the table, identifying a specific row by index label, and setting its population, in this case to a None value, which Pandas interprets as a NaN (missing value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[688,'Population'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter for values that are Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Population'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more commonly, filter out the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[df['Population'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also fill the missing values with a specific number. In this case let's just put the value back that we overwrote earlier with a NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[688,'Population']  = 133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[688]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more common use case for this approach would be to replace special values like -9999 with Nan, or vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Manipulation in DataFrames\n",
    "\n",
    "We saw last time how to use the str attribute to do a bit of string manipulation. Below let's create a new column, called 'state', populated by getting the last element in the geodisplay field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['state'] = df['geodisplay'].str.split(',').str[2]\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same approach to add a 'county' column to our dataframe, pulling the values from the geodisplay column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['county'] = df['geodisplay'].str.split(',').str[1].str.replace(' County', '')\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a different way to get the County values extrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['county'] = df['geodisplay'].str.split(',').str[1].str.split(' ').str[1:-1].str.join(' ')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to make sure our approach preserves two word County names.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['geodisplay'].str.contains('El Dorado County')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the full list of unique county names is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['county'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check-in time: How would you count how many census tracts are in each county? http://bitly.com/cp255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a census tract column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['Tract'] = df['geodisplay'].str.split(',').str[0].str.split().str[2]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could you extract Tract from the GEOIDLONG column instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Tract'] = df['GEOIDLONG'].str[-6:-2] + '.' + df['GEOIDLONG'].str[-2:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging / Joining Dataframes\n",
    "\n",
    "Pandas has a very powerful set of methods to merge dataframes. Let's review some on a simple example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorite_numbers = pd.DataFrame(\n",
    "    [[\"Paul\", 42],\n",
    "     [\"Paul\", 3.14],\n",
    "     [\"Arezoo\", 7],\n",
    "     [\"Arezoo\", 9],\n",
    "     [\"Sam\", 3],\n",
    "     [\"Geoff\", np.NaN]], columns = [\"Name\", \"Number\"])\n",
    "email_addr = pd.DataFrame(\n",
    "    [[\"Paul\", \"waddell@berkeley.edu\"],\n",
    "     [\"Arezoo\", \"arezoo.bz@berkeley.edu\"],\n",
    "     [\"Sam\", \"maurer@berkeley.edu\"],\n",
    "     [\"Geoff\", \"gboeing@berkeley.edu\"],\n",
    "     [\"Max\", \"magardner@berkeley.edu\"]], columns = [\"Name\", \"Email\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first table contains favorite numbers of some famous people.  (The numbers have been changed to protect their identity.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorite_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second table contains the individuals email addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_addr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many ways to join the data\n",
    "\n",
    "There are actually many ways you could imagine combining data from both of these tables.  In the following we work through a few example methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge\n",
    "\n",
    "Probably the most general and standard way to join tables in pandas is to use the merge function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(favorite_numbers, email_addr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the above join that:\n",
    "1. The **`Name`** column is used to define which records match from each table.  Pandas will by default join on any matching column names.\n",
    "1. Only the records that occur in both tables are included in the final table.  This is called an inner join.\n",
    "1. Joey occurs 4 times since the name Joey had two email addresses and two favorite numbers.\n",
    "\n",
    "Many of the joins you will do in data science will either be inner joins or left joins (see below). \n",
    "\n",
    "We could be more specific about the join using the following additional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.merge(favorite_numbers, email_addr, on=\"Name\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left Merge\n",
    "\n",
    "A left join will keep all the entries in the left table even if they have no matching entry in the right table.  For example we didn't have Nhi or Bob's email address and so they appear as missing values rather than being dropped from the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(favorite_numbers, email_addr, on=\"Name\",how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer Merge\n",
    "\n",
    "The outer join keeps entries in both tables even if they don't have a match in the other and substitutes NaN for missing values. \n",
    "\n",
    "pd.merge(favorite_numbers, email_addr, on=\"Name\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join\n",
    "\n",
    "Pandas also provides a join function which joins two tables on their index.  This function also let's you specify what kind of join you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "favorite_numbers.set_index(\"Name\").join(email_addr.set_index(\"Name\"), how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More resources to learn about merges and joins\n",
    "\n",
    "These are powerful tools and there are some subtleties in using them.  I recommend doing a fair amount of reading and a lot of practicing to get these ideas and the accompanying syntax internalized into your programming toolkit.\n",
    "\n",
    "Here is another good (and short) tutorial: https://chrisalbon.com/python/pandas_join_merge_dataframe.html\n",
    "\n",
    "And as always, **ReadTheDocs**: http://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Merge on Real Datasets and Using String Operations\n",
    "\n",
    "Now that we have some concepts and syntax under control, let's try using them on some larger data tables from the Census, and learn how to use string operations to work with these data.\n",
    "\n",
    "### County level Census Data for the U.S.\n",
    "\n",
    "Lets say we want to analyze county level population trends. I downloaded population two estimates tables for U.S. Counties: one contains 2000-2010, and the other has 2010-2016.  Lets load them and do some data exploration and manipulation to get a merged county level population series for the 2000 - 2016 period.\n",
    "\n",
    "One thing that you will encounter as you get csv files from various locations, is that the character encoding might be unusual, and require setting the encoding on the read statement.  Full documentation on this is available here:\n",
    "https://docs.python.org/3/library/codecs.html#standard-encodings\n",
    "\n",
    "Some of the most common ones are:\n",
    "\n",
    "Latin1\n",
    "iso-8859-1\n",
    "utf_8\n",
    "utf_16\n",
    "utf_32\n",
    "\n",
    "Let's see what happens when you try loading a csv file with an encoding issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co00 = pd.read_csv('data/co-est00int-tot.csv')\n",
    "co00.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co00 = pd.read_csv('data/co-est00int-tot.csv', encoding='Latin1')\n",
    "co00.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "co16 = pd.read_csv('data/co-est2016-alldata.csv', encoding='latin1')\n",
    "co16.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**116 columns!?!  We don't want all of those!  How can we keep just the columns we want? The ones containing POPESTIMATE, and the initial geographic columns?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co16s = co16.loc[:,:'POPESTIMATE2016']\n",
    "co16s.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahhh, that's better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "co16s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But wait... what's going on here?  There seem to be at least two SUMLEV values and it looks suspiciously like there are State level summaries of POPESTIMATES embedded in this County-level file.  Let's check how many records there are for each SUMLEV in the file...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "co16s['SUMLEV'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, OK, so we have 51 SUMLEV 40, which seem to be States, plus one... let's pull those into a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st00 = co00[co00['SUMLEV']==40]\n",
    "st00.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st00.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st00['STNAME'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is that 51st entry in the set of States?\n",
    "\n",
    "Now let's pull the state records out of the 2016 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st16 = co16s[co16s['SUMLEV']==40]\n",
    "st16.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now how can we merge the 2000 and 2016 state dataframes?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stjoin = pd.merge(st00,st16, on='STATE')\n",
    "stjoin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks like it worked!  But what are all those columns with _x and _y suffixes?**\n",
    "\n",
    "Seems like a mess to keep all those duplicate columns and have them get renamed like this... what to do...?\n",
    "\n",
    "Maybe we could find the columns that are different in the second dataframe and just add those to the first dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols_to_use = list(st16.columns.difference(st00.columns))\n",
    "cols_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would it work if we tried to join these two using this as the list of columns from ST16?  What would be the join column? Seems like that would be pretty handy to have..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols_to_use.append('STATE')\n",
    "cols_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a cleaner merge using STATE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stjoin2 = pd.merge(st00, st16[cols_to_use], on='STATE')\n",
    "stjoin2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking pretty good.  But there are still a lot of columns we really don't want.  Let's drop those, and set the index to STNAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stjoin3 = stjoin2.drop(['SUMLEV','STATE','REGION','DIVISION','COUNTY','ESTIMATESBASE2000','ESTIMATESBASE2010','CENSUS2010POP','CTYNAME'], axis=1)\n",
    "stjoin3.set_index('STNAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stjoin3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much nicer.  But I really don't like those column names.  Can't we just make them simple years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stjoin4 = stjoin3\n",
    "stjoin4.columns = list(stjoin3.columns[0:1])+list(range(2000,2017))\n",
    "stjoin4 = stjoin4.set_index('STNAME')\n",
    "stjoin4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might also be handy to transpose the data so that the rows become columns and vice versa.  Let's do that and select just a couple of states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ST = stjoin4.transpose()\n",
    "ST[['California','New York']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe a quick plot of the population trends in these states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(ST[['California', 'Texas']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for those of you who love your Excel for making charts, since mine is pretty ugly, go for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('data/state_population.xlsx')\n",
    "stjoin4.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Cleaning\n",
    "\n",
    "**Let's load some crime call events from the Berkeley Police Department and explore those data...  this is an example of having to wrangle some messy, real world data.  It's advanced work that you now can handle in Python!**\n",
    "\n",
    "For example lets say we want to find out how many vandalism calls there are each day of the week. Let's load some data and figure out how to answer that.\n",
    "\n",
    "https://data.cityofberkeley.info/Public-Safety/Berkeley-PD-Calls-for-Service/k2nh-s5h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calls = pd.read_csv('data/Berkeley_PD_-_Calls_for_Service.csv')\n",
    "calls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also load a lookup for CVDOW (Day of Week) to the day names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvdow = pd.read_csv(\"data/cvdow.csv\")\n",
    "cvdow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many calls are in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary observations on the data?\n",
    "\n",
    "1. `EVENTDT` -- Contain the incorrect time stamp (all the times are 12:00 am)\n",
    "1. `EVENTTM` -- Contains the time in 24 hour format (What timezone?)\n",
    "1. `InDbDate` -- Appears to be correctly formatted and appears awfully consistent in time.\n",
    "1. **`Block_Location` -- Errr, what a mess!  newline characters, and Geocoordinates all merged!!  Fortunately, this field was \"quoted\" otherwise we would have had trouble parsing the file. (why?)**\n",
    "1. `BLKADDR` -- This appears to be the address in Block Location.\n",
    "1. `City` and `State` seem redundant given this is supposed to be the city of Berkeley dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new = pd.merge(calls,cvdow, on='CVDOW', how='inner')\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we find out how many Vandalism calls happen by Day of the week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new['Day'][new['OFFENSE']=='VANDALISM'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new['Block_Location'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new['Block_Location'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new['Block_Location'].str.find('(').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for label in new.Block_Location.index:\n",
    "    start=new.Block_Location[label].find('(') + 1\n",
    "    end= start+9\n",
    "    new.loc[label,'Lat'] = new.Block_Location[label][start:end].strip(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now do this to add a Long column to the dataframe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in new.Block_Location.index:\n",
    "    start=new.Block_Location[label].find('-')\n",
    "    end= new.Block_Location[label].find(')')\n",
    "    new.loc[label,'Lon'] = new.Block_Location[label][start:end].strip(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could you check whether city and state are all redundant (since this is supposed to be City of Berkeley data)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new['City'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new['State'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If they are duplicated, delete those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new.drop(['State', 'City'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix the date time to be able to parse the dates and work with them. Pandas has very helpful functionality using a datetime datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new['EVENTDT'].to_datetime()\n",
    "new['DATE'] = pd.to_datetime(new['EVENTDT'])\n",
    "new['MONTH'] = new['DATE'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
